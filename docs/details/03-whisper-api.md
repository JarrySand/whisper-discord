# ğŸ“˜ Whisper API ã‚µãƒ¼ãƒãƒ¼ è©³ç´°ä»•æ§˜æ›¸

> **é–¢é€£**: [spec.md](../spec.md) - å…¨ä½“æ¦‚è¦ | [05-integration.md](./05-integration.md) - é€£æºä»•æ§˜

---

## 1. æ¦‚è¦

éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å—ã‘å–ã‚Šã€Whisper ãƒ¢ãƒ‡ãƒ«ã§æ–‡å­—èµ·ã“ã—ã‚’è¡Œã†HTTP APIã‚µãƒ¼ãƒãƒ¼ã€‚

### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

| é …ç›® | æŠ€è¡“ |
|------|------|
| è¨€èª | Python 3.10+ |
| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | FastAPI |
| Whisperãƒ©ã‚¤ãƒ–ãƒ©ãƒª | faster-whisper (CTranslate2) |
| ãƒ¢ãƒ‡ãƒ« | large-v3 |
| ASGIã‚µãƒ¼ãƒãƒ¼ | Uvicorn |

### è¨­è¨ˆæ€æƒ³

- **å˜ä¸€è²¬ä»»**: éŸ³å£° â†’ ãƒ†ã‚­ã‚¹ãƒˆå¤‰æ›ã®ã¿
- **ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ç‹¬ç«‹
- **è©±è€…è­˜åˆ¥ãªã—**: è©±è€…æƒ…å ±ã¯ Bot å´ã®è²¬å‹™

---

## 2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
whisper-api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py               # FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py         # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾©
â”‚   â”‚   â””â”€â”€ schemas.py        # Pydantic ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py         # è¨­å®šç®¡ç†
â”‚   â”‚   â””â”€â”€ logging.py        # ãƒ­ã‚°è¨­å®š
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ whisper.py        # Whisperæ¨è«–ã‚µãƒ¼ãƒ“ã‚¹
â”‚   â”‚   â””â”€â”€ audio.py          # éŸ³å£°å‰å‡¦ç†
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ file.py           # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
â”œâ”€â”€ models/                   # Whisperãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
â”œâ”€â”€ temp/                     # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env.example
```

---

## 3. API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆä»•æ§˜

### 3.1 POST `/transcribe`

**æ¦‚è¦**: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```http
POST /transcribe HTTP/1.1
Content-Type: multipart/form-data

--boundary
Content-Disposition: form-data; name="audio_file"; filename="segment.ogg"
Content-Type: audio/ogg

(binary audio data)
--boundary
Content-Disposition: form-data; name="user_id"

123456789012345678
--boundary
Content-Disposition: form-data; name="username"

Alice
--boundary
Content-Disposition: form-data; name="start_ts"

1733389200123
--boundary
Content-Disposition: form-data; name="end_ts"

1733389203954
--boundary--
```

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | å¿…é ˆ | èª¬æ˜ |
|-----------|-----|------|------|
| `audio_file` | File | âœ… | éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ« (OGG/WAV/MP3) |
| `user_id` | string | âœ… | Discord User ID |
| `username` | string | âœ… | Discord Username |
| `display_name` | string | âŒ | ã‚µãƒ¼ãƒãƒ¼è¡¨ç¤ºå |
| `start_ts` | number | âœ… | é–‹å§‹ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— (Unix ms) |
| `end_ts` | number | âœ… | çµ‚äº†ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— (Unix ms) |
| `language` | string | âŒ | è¨€èªãƒ’ãƒ³ãƒˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "ja") |

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ (æˆåŠŸ: 200)

```json
{
  "success": true,
  "data": {
    "user_id": "123456789012345678",
    "username": "Alice",
    "display_name": "ã‚¢ãƒªã‚¹",
    "text": "ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã¯ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ã€‚",
    "start_ts": 1733389200123,
    "end_ts": 1733389203954,
    "duration_ms": 3831,
    "language": "ja",
    "confidence": 0.95,
    "processing_time_ms": 1250
  }
}
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ (ã‚¨ãƒ©ãƒ¼: 4xx/5xx)

```json
{
  "success": false,
  "error": {
    "code": "AUDIO_TOO_SHORT",
    "message": "Audio duration is less than minimum required (500ms)",
    "details": {
      "duration_ms": 320,
      "min_duration_ms": 500
    }
  }
}
```

---

### 3.2 POST `/transcribe/batch`

**æ¦‚è¦**: è¤‡æ•°ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†

#### ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

```http
POST /transcribe/batch HTTP/1.1
Content-Type: multipart/form-data

--boundary
Content-Disposition: form-data; name="files"; filename="segment1.ogg"
(binary)
--boundary
Content-Disposition: form-data; name="files"; filename="segment2.ogg"
(binary)
--boundary
Content-Disposition: form-data; name="metadata"

[
  {"user_id": "123", "username": "Alice", "start_ts": 1000, "end_ts": 2000},
  {"user_id": "456", "username": "Bob", "start_ts": 1500, "end_ts": 2500}
]
--boundary--
```

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "success": true,
  "data": {
    "results": [
      {
        "index": 0,
        "success": true,
        "text": "ã“ã‚“ã«ã¡ã¯",
        "user_id": "123",
        ...
      },
      {
        "index": 1,
        "success": true,
        "text": "ã¯ã„ã€ã‚ˆã‚ã—ã",
        "user_id": "456",
        ...
      }
    ],
    "total_processing_time_ms": 2100
  }
}
```

---

### 3.3 GET `/health`

**æ¦‚è¦**: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "status": "healthy",
  "model_loaded": true,
  "model_name": "large-v3",
  "device": "cuda",
  "compute_type": "float16",
  "uptime_seconds": 3600,
  "requests_processed": 150,
  "avg_processing_time_ms": 1100
}
```

---

### 3.4 GET `/status`

**æ¦‚è¦**: è©³ç´°ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹

```json
{
  "server": {
    "version": "1.0.0",
    "uptime": "1:00:00",
    "python_version": "3.10.12"
  },
  "model": {
    "name": "large-v3",
    "loaded": true,
    "device": "cuda",
    "compute_type": "float16",
    "vram_used_mb": 3200
  },
  "stats": {
    "total_requests": 150,
    "successful_requests": 148,
    "failed_requests": 2,
    "avg_processing_time_ms": 1100,
    "total_audio_processed_seconds": 450
  },
  "queue": {
    "pending": 0,
    "processing": 1,
    "max_concurrent": 4
  }
}
```

---

## 4. Pydantic ã‚¹ã‚­ãƒ¼ãƒ

```python
# api/schemas.py
from pydantic import BaseModel, Field
from typing import Optional, List

class TranscribeRequest(BaseModel):
    user_id: str = Field(..., description="Discord User ID")
    username: str = Field(..., description="Discord Username")
    display_name: Optional[str] = Field(None, description="Server display name")
    start_ts: int = Field(..., description="Start timestamp (Unix ms)")
    end_ts: int = Field(..., description="End timestamp (Unix ms)")
    language: str = Field("ja", description="Language hint")

class TranscriptionResult(BaseModel):
    user_id: str
    username: str
    display_name: Optional[str]
    text: str
    start_ts: int
    end_ts: int
    duration_ms: int
    language: str
    confidence: float
    processing_time_ms: int

class TranscribeResponse(BaseModel):
    success: bool
    data: Optional[TranscriptionResult] = None
    error: Optional[dict] = None

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    model_name: str
    device: str
    compute_type: str
    uptime_seconds: int
    requests_processed: int
    avg_processing_time_ms: float

class BatchMetadata(BaseModel):
    user_id: str
    username: str
    display_name: Optional[str] = None
    start_ts: int
    end_ts: int
    language: str = "ja"
```

---

## 5. Whisper ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè£…

```python
# services/whisper.py
from faster_whisper import WhisperModel
from typing import Optional, Tuple
import time
import os

class WhisperService:
    def __init__(self, config: 'WhisperConfig'):
        self.config = config
        self.model: Optional[WhisperModel] = None
        self.load_time: Optional[float] = None
        self.stats = TranscriptionStats()
    
    def load_model(self) -> None:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆèµ·å‹•æ™‚ã«1å›ã®ã¿ï¼‰"""
        start = time.time()
        
        self.model = WhisperModel(
            model_size_or_path=self.config.model_name,
            device=self.config.device,
            compute_type=self.config.compute_type,
            download_root=self.config.model_cache_dir,
            local_files_only=self.config.local_files_only,
        )
        
        self.load_time = time.time() - start
        print(f"Model loaded in {self.load_time:.2f}s")
    
    def transcribe(
        self,
        audio_path: str,
        language: str = "ja",
    ) -> Tuple[str, float]:
        """
        éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–‡å­—èµ·ã“ã—
        
        Returns:
            Tuple[str, float]: (æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆ, ä¿¡é ¼åº¦)
        """
        if self.model is None:
            raise RuntimeError("Model not loaded")
        
        start = time.time()
        
        segments, info = self.model.transcribe(
            audio_path,
            language=language,
            task="transcribe",
            beam_size=self.config.beam_size,
            best_of=self.config.best_of,
            temperature=self.config.temperature,
            vad_filter=self.config.vad_filter,
            vad_parameters=self.config.vad_parameters,
        )
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµåˆ
        text_parts = []
        total_confidence = 0.0
        segment_count = 0
        
        for segment in segments:
            text_parts.append(segment.text.strip())
            total_confidence += segment.avg_logprob
            segment_count += 1
        
        text = " ".join(text_parts).strip()
        
        # å¹³å‡ä¿¡é ¼åº¦ã‚’è¨ˆç®—ï¼ˆlog probã‹ã‚‰å¤‰æ›ï¼‰
        avg_confidence = 0.0
        if segment_count > 0:
            avg_logprob = total_confidence / segment_count
            # logprobã‚’0-1ã®ä¿¡é ¼åº¦ã«å¤‰æ›
            avg_confidence = min(1.0, max(0.0, 1.0 + avg_logprob / 3))
        
        processing_time = time.time() - start
        self.stats.record(processing_time, len(text) > 0)
        
        return text, avg_confidence
    
    def is_ready(self) -> bool:
        return self.model is not None


class TranscriptionStats:
    def __init__(self):
        self.total_requests = 0
        self.successful_requests = 0
        self.total_processing_time = 0.0
    
    def record(self, processing_time: float, success: bool):
        self.total_requests += 1
        self.total_processing_time += processing_time
        if success:
            self.successful_requests += 1
    
    @property
    def avg_processing_time(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return self.total_processing_time / self.total_requests
```

---

## 6. è¨­å®šç®¡ç†

```python
# core/config.py
from pydantic_settings import BaseSettings
from typing import Optional, Dict, Any

class WhisperConfig(BaseSettings):
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    model_name: str = "large-v3"
    device: str = "auto"  # "auto", "cuda", "cpu"
    compute_type: str = "auto"  # "auto", "float16", "int8", "float32"
    model_cache_dir: str = "./models"
    local_files_only: bool = False
    
    # æ¨è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    beam_size: int = 5
    best_of: int = 5
    temperature: float = 0.0
    
    # VAD (Voice Activity Detection)
    vad_filter: bool = True
    vad_parameters: Dict[str, Any] = {
        "threshold": 0.5,
        "min_speech_duration_ms": 250,
        "min_silence_duration_ms": 100,
        "speech_pad_ms": 30,
    }
    
    class Config:
        env_prefix = "WHISPER_"


class ServerConfig(BaseSettings):
    # ã‚µãƒ¼ãƒãƒ¼è¨­å®š
    host: str = "0.0.0.0"
    port: int = 8000
    workers: int = 1  # Whisperã¯å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹æ¨å¥¨
    
    # åˆ¶é™
    max_file_size_mb: int = 25
    max_audio_duration_seconds: int = 300  # 5åˆ†
    min_audio_duration_ms: int = 500
    request_timeout_seconds: int = 120
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
    temp_dir: str = "./temp"
    cleanup_interval_seconds: int = 300
    
    class Config:
        env_prefix = "SERVER_"


class Config(BaseSettings):
    whisper: WhisperConfig = WhisperConfig()
    server: ServerConfig = ServerConfig()
    
    # ãƒ­ã‚°
    log_level: str = "INFO"
    log_format: str = "json"  # "json" or "text"
    
    class Config:
        env_file = ".env"
```

---

## 7. FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³

```python
# main.py
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import tempfile
import os
import time

from core.config import Config
from services.whisper import WhisperService
from api.schemas import TranscribeResponse, TranscriptionResult, HealthResponse

config = Config()
whisper_service = WhisperService(config.whisper)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # èµ·å‹•æ™‚: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    print("Loading Whisper model...")
    whisper_service.load_model()
    print("Model loaded successfully")
    
    yield
    
    # çµ‚äº†æ™‚: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    print("Shutting down...")

app = FastAPI(
    title="Whisper Transcription API",
    version="1.0.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/transcribe", response_model=TranscribeResponse)
async def transcribe(
    audio_file: UploadFile = File(...),
    user_id: str = Form(...),
    username: str = Form(...),
    display_name: str = Form(None),
    start_ts: int = Form(...),
    end_ts: int = Form(...),
    language: str = Form("ja"),
):
    start_time = time.time()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    content = await audio_file.read()
    file_size_mb = len(content) / (1024 * 1024)
    
    if file_size_mb > config.server.max_file_size_mb:
        raise HTTPException(400, f"File too large: {file_size_mb:.2f}MB")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    suffix = os.path.splitext(audio_file.filename or ".ogg")[1]
    with tempfile.NamedTemporaryFile(
        delete=False,
        suffix=suffix,
        dir=config.server.temp_dir,
    ) as tmp:
        tmp.write(content)
        tmp_path = tmp.name
    
    try:
        # æ–‡å­—èµ·ã“ã—
        text, confidence = whisper_service.transcribe(tmp_path, language)
        
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        return TranscribeResponse(
            success=True,
            data=TranscriptionResult(
                user_id=user_id,
                username=username,
                display_name=display_name,
                text=text,
                start_ts=start_ts,
                end_ts=end_ts,
                duration_ms=end_ts - start_ts,
                language=language,
                confidence=confidence,
                processing_time_ms=processing_time_ms,
            )
        )
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        os.unlink(tmp_path)


@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(
        status="healthy" if whisper_service.is_ready() else "loading",
        model_loaded=whisper_service.is_ready(),
        model_name=config.whisper.model_name,
        device=config.whisper.device,
        compute_type=config.whisper.compute_type,
        uptime_seconds=int(time.time() - app.state.start_time) if hasattr(app.state, 'start_time') else 0,
        requests_processed=whisper_service.stats.total_requests,
        avg_processing_time_ms=whisper_service.stats.avg_processing_time * 1000,
    )
```

---

## 8. ãƒ‡ãƒã‚¤ã‚¹é¸æŠãƒ­ã‚¸ãƒƒã‚¯

```python
# services/device.py
import torch

def detect_device() -> str:
    """åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’æ¤œå‡º"""
    if torch.cuda.is_available():
        return "cuda"
    elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"  # Apple Silicon
    else:
        return "cpu"

def detect_compute_type(device: str) -> str:
    """ãƒ‡ãƒã‚¤ã‚¹ã«é©ã—ãŸè¨ˆç®—ç²¾åº¦ã‚’é¸æŠ"""
    if device == "cuda":
        # CUDAã®å ´åˆã€GPUã®èƒ½åŠ›ã«å¿œã˜ã¦é¸æŠ
        capability = torch.cuda.get_device_capability()
        if capability[0] >= 7:  # Voltaä»¥é™
            return "float16"
        else:
            return "int8"
    elif device == "mps":
        return "float16"
    else:
        return "int8"  # CPUã¯int8ãŒåŠ¹ç‡çš„
```

---

## 9. ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ä¸€è¦§

| ã‚³ãƒ¼ãƒ‰ | HTTP Status | èª¬æ˜ |
|--------|-------------|------|
| `AUDIO_TOO_SHORT` | 400 | éŸ³å£°ãŒçŸ­ã™ãã‚‹ï¼ˆ< 500msï¼‰ |
| `AUDIO_TOO_LONG` | 400 | éŸ³å£°ãŒé•·ã™ãã‚‹ï¼ˆ> 5åˆ†ï¼‰ |
| `FILE_TOO_LARGE` | 400 | ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºè¶…é |
| `INVALID_FORMAT` | 400 | éå¯¾å¿œã®éŸ³å£°å½¢å¼ |
| `MISSING_PARAMETER` | 400 | å¿…é ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸è¶³ |
| `MODEL_NOT_LOADED` | 503 | ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰ |
| `TRANSCRIPTION_FAILED` | 500 | æ–‡å­—èµ·ã“ã—å¤±æ•— |
| `TIMEOUT` | 504 | å‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ |

---

## 10. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¬ã‚¤ãƒ‰

### 10.1 ãƒ¢ãƒ‡ãƒ«åˆ¥æ€§èƒ½æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ« | VRAM | CPUå‡¦ç†æ™‚é–“ (1åˆ†éŸ³å£°) | GPUå‡¦ç†æ™‚é–“ | ç²¾åº¦ |
|--------|------|---------------------|-------------|------|
| tiny | 1GB | 5ç§’ | 1ç§’ | â˜…â˜…â˜†â˜†â˜† |
| base | 1GB | 10ç§’ | 2ç§’ | â˜…â˜…â˜…â˜†â˜† |
| small | 2GB | 20ç§’ | 3ç§’ | â˜…â˜…â˜…â˜…â˜† |
| medium | 5GB | 40ç§’ | 5ç§’ | â˜…â˜…â˜…â˜…â˜† |
| large-v3 | 10GB | 60ç§’ | 8ç§’ | â˜…â˜…â˜…â˜…â˜… |

### 10.2 CPUæœ€é©åŒ–

```python
# CPUä½¿ç”¨æ™‚ã®æœ€é©è¨­å®š
config = WhisperConfig(
    device="cpu",
    compute_type="int8",  # int8é‡å­åŒ–ã§é«˜é€ŸåŒ–
    beam_size=1,          # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒç„¡åŠ¹åŒ–
    best_of=1,            # å€™è£œã‚’1ã¤ã«
)
```

### 10.3 GPUæœ€é©åŒ–

```python
# GPUä½¿ç”¨æ™‚ã®æœ€é©è¨­å®š
config = WhisperConfig(
    device="cuda",
    compute_type="float16",
    beam_size=5,
    best_of=5,
)
```

---

## 11. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```
# requirements.txt
fastapi==0.109.0
uvicorn[standard]==0.25.0
python-multipart==0.0.6
pydantic==2.5.3
pydantic-settings==2.1.0

# Whisper
faster-whisper==1.0.0
torch>=2.0.0
torchaudio>=2.0.0

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
python-dotenv==1.0.0
```

### GPUç‰ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# CUDA 11.x
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.x
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121
```

---

## 12. Docker æ§‹æˆ

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# FFmpeg ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
RUN apt-get update && apt-get install -y ffmpeg && rm -rf /var/lib/apt/lists/*

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ãƒ”ãƒ¼
COPY src/ ./src/

# ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
RUN mkdir -p /app/models /app/temp

ENV WHISPER_MODEL_CACHE_DIR=/app/models
ENV SERVER_TEMP_DIR=/app/temp

EXPOSE 8000

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

```yaml
# docker-compose.yml
version: '3.8'

services:
  whisper-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./temp:/app/temp
    environment:
      - WHISPER_MODEL_NAME=large-v3
      - WHISPER_DEVICE=cpu
      - WHISPER_COMPUTE_TYPE=int8
    deploy:
      resources:
        limits:
          memory: 8G
```

---

## 13. æ¬¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [04-output-logging.md](./04-output-logging.md) - å‡ºåŠ›ä»•æ§˜
- [05-integration.md](./05-integration.md) - Botâ‡”APIé€£æº

